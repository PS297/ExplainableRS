{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5416effd-fa71-4cb8-b673-46e2c7497246",
   "metadata": {},
   "source": [
    "## Importing Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5da2f-6981-408f-8f62-c22e957505dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from pymoo.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import shap\n",
    "from captum.attr import IntegratedGradients\n",
    "from shap import DeepExplainer, summary_plot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from captum.attr import GradientShap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d53313-c536-4d76-913c-bd88f4ac094a",
   "metadata": {},
   "source": [
    "## Defining Datapath and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45ce60-ef67-4d54-acb7-dafc5b6706ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH_Film = \"FilmTrust\"\n",
    "\n",
    "Ratings = pd.read_csv(f'{DATAPATH_Film}/ratings.txt', sep=' ', names=['userId', 'itemId', 'rating']) ## Extracting the ratings from the dataset\n",
    "Ratings.drop_duplicates(inplace=True)\n",
    "Ratings = Ratings.groupby(['userId', 'itemId'], as_index=False)['rating'].mean() ## Grouping the ratings by matching with userID and itemID\n",
    "print(\"Ratings shape in the FilmTrust Dataset:\", Ratings.shape)\n",
    "\n",
    "Interaction_Count = Ratings.groupby('userId')['itemId'].nunique().reset_index() ## Determinin the intrecation cound of users towards item\n",
    "Interaction_Count.rename(columns={'itemId': 'A_Interaction_Count'}, inplace=True)  ## Adding colun of Interaction Count\n",
    "\n",
    "Ratings = Ratings.merge(Interaction_Count, on='userId', how='left') ## Adding Interaction Count into the rating\n",
    "\n",
    "Trust = pd.read_csv(f'{DATAPATH_Film}/trust.txt', sep=' ', names=['userId', 'userId_1', 'trust']) ## Loading Trust Fature given in the dataset\n",
    "print(\"Trust shape in the FilmTrust Dataset:\", Trust.shape)\n",
    "print(Ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac64f0b-bd54-430d-aee7-8a658b357c9e",
   "metadata": {},
   "source": [
    "## Extracting More fatures using Aggregating provided infromation in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba0515-7d41-4d0f-8185-0d540c3953ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trust_Agg_O = Trust.groupby('userId')['trust'].agg(['sum']).reset_index() ## Aggregating the trust of other users\n",
    "Trust_Agg_O.columns = ['userId', 'A_Sum_Trust_UsersO']\n",
    "\n",
    "Trust_Agg_S = Trust.groupby('userId_1')['trust'].agg(['sum']).reset_index() ## Aggregating the trust of sel\n",
    "Trust_Agg_S.columns = ['userId', 'A_Sum_Trust_UsersS']\n",
    "\n",
    "Ratings = pd.merge(Ratings, Trust_Agg_S, on='userId', how='left') ## Merging the aggregated feature infromation to the rating\n",
    "Ratings = pd.merge(Ratings, Trust_Agg_O, on='userId', how='left')\n",
    "print(\"Ratings shape after merging trust features:\", Ratings.shape)\n",
    "\n",
    "D_Ratings = Ratings.copy() ## Copying the ratings data\n",
    "\n",
    "Users = D_Ratings['userId'].unique()\n",
    "Users = np.random.permutation(Users)\n",
    "Normal_Seg = 0.9\n",
    "Num_Normal = int(np.round(len(Users) * Normal_Seg))\n",
    "Users_Normal = Users[:Num_Normal]\n",
    "\n",
    "D_Normal = D_Ratings[D_Ratings['userId'].isin(Users_Normal)]\n",
    "print(\"Normal Users Shape:\", D_Normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4293c-d9bd-4756-9514-df175e2ce670",
   "metadata": {},
   "source": [
    "## Segregating the Dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ebc8e-9491-4bda-9c51-d14ef0c7412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temp_train, Temp_valid = train_test_split(D_Normal, test_size = 0.2, train_size = 0.8, random_state=42, shuffle=True)\n",
    "Temp_train['random_dstype'] = 'train'\n",
    "Temp_valid['random_dstype'] = 'test'\n",
    "\n",
    "D_Normal = pd.concat([Temp_train, Temp_valid], axis=0)\n",
    "print(D_Normal['random_dstype'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d65769-7bb9-41d8-b6e1-23074debb3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_train = D_Normal[D_Normal['random_dstype'] == 'train'].copy()\n",
    "D_valid = D_Normal[D_Normal['random_dstype'] == 'test'].copy()\n",
    "\n",
    "assert D_train['userId'].isin(Users_Normal).all(), \"Train set contains cold-start users!\"\n",
    "assert D_valid['userId'].isin(Users_Normal).all(), \"Valid set contains cold-start users!\"\n",
    "\n",
    "print(D_train.shape)\n",
    "print(D_valid.shape)\n",
    "\n",
    "D_train['flag_train'] = 1\n",
    "D_valid['flag_train'] = 0\n",
    "\n",
    "D_train, D_valid = train_test_split(D_Normal, test_size = 0.1, train_size = 0.9, random_state=42, shuffle=True)\n",
    "User_Features = D_train.columns[D_train.columns.str.startswith('A_')]\n",
    "\n",
    "print(\"Final Train shape:\", D_train.shape)\n",
    "print(\"Final Valid shape :\", D_valid.shape)\n",
    "\n",
    "for feature in User_Features:\n",
    "    D_train[f'A_na_{feature[2:]}'] = D_train[feature].isnull()\n",
    "    D_train[feature].fillna(0, inplace=True)\n",
    "\n",
    "for feature in User_Features:\n",
    "    D_valid[f'A_na_{feature[2:]}'] = D_valid[feature].isnull()\n",
    "    D_valid[feature].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(D_train.head())\n",
    "print(\"Final Train shape:\", D_train.shape)\n",
    "print(\"Final Valid shape :\", D_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c323-3daa-427d-9564-3ffc9610d9f5",
   "metadata": {},
   "source": [
    "## Saving the Training and testin Data into CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca041a15-df00-4881-a264-f6ecd2d437a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train.to_csv(\"D_train_FilmTrust.csv\", index=False)\n",
    "D_valid.to_csv(\"D_valid_FilmTrust.csv\", index=False)\n",
    "\n",
    "print(\"Final Train shape:\", D_train.shape)\n",
    "print(\"Final Valid shape :\", D_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f54b41-7fcf-4e2f-b794-e875543a36d0",
   "metadata": {},
   "source": [
    "## Embedded Feature-wise Linear Modulation (FiLM) DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc4d2c-1b48-46f2-a7f9-23b97b9398e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbFilm(nn.Module):\n",
    "    def __init__(self, N_Users, N_Items, Dense_Size, Hidden, Dropout_emb, Dropouts):\n",
    "        super().__init__()\n",
    "        self.Emb_User = nn.Embedding(N_Users + 1, 25)\n",
    "        self.Emb_Item = nn.Embedding(N_Items + 1, 25)\n",
    "        self.Hidden = Hidden\n",
    "        self.Emb_Dropout = nn.Dropout(Dropout_emb)\n",
    "\n",
    "        self.LinC1 = nn.Sequential(\n",
    "            nn.Linear(Dense_Size, Hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(Hidden[1]),\n",
    "            nn.Dropout(Dropouts[1])\n",
    "        )\n",
    "        self.Emb_Projection = nn.Linear(50, Hidden[1])\n",
    "\n",
    "        self.Film_Gamma1 = nn.Linear(2 * Hidden[1], Hidden[1])\n",
    "        self.Film_Gamma2 = nn.Linear(2 * Hidden[1], Hidden[1])\n",
    "        self.Film_Beta = nn.Linear(2 * Hidden[1], Hidden[1])\n",
    "\n",
    "        self.LinC2 = nn.Sequential(\n",
    "            nn.Linear(Hidden[1], Hidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(Hidden[2]),\n",
    "            nn.Dropout(Dropouts[2])\n",
    "        )\n",
    "        \n",
    "        self.LinC3 = nn.Linear(Hidden[2], 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def predict_fn(self, Side_np, device):\n",
    "        Side_tensor = torch.tensor(Side_np, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            Users = torch.zeros(Side_tensor.size(0), dtype=torch.long).to(device)\n",
    "            Items = torch.zeros_like(Users)\n",
    "            User_EmbV = self.Emb_User(Users)\n",
    "            Item_EmbV = self.Emb_Item(Items)\n",
    "\n",
    "            EmbV_Concat = torch.cat([User_EmbV, Item_EmbV], dim=1)\n",
    "            EmbV_Proj = self.Emb_Projection(EmbV_Concat)\n",
    "\n",
    "            Out1 = self.LinC1(Side_tensor)\n",
    "            Cat_Out_UISFeat = torch.cat([EmbV_Proj, Out1], dim=1)\n",
    "\n",
    "            Gamma1 = self.Film_Gamma1(Cat_Out_UISFeat)\n",
    "            Gamma2 = self.Film_Gamma2(Cat_Out_UISFeat)\n",
    "            Beta = self.Film_Beta(Cat_Out_UISFeat)\n",
    "\n",
    "            Aggregated_Film = Gamma1 * EmbV_Proj + Gamma2 * Out1 + Beta\n",
    "            Out2 = self.LinC2(Aggregated_Film)\n",
    "            Out_Rat = self.LinC3(Out2)\n",
    "            return torch.clamp(Out_Rat, 1.0, 5.0).cpu().numpy()\n",
    "\n",
    "    def forward(self, Xb, Yb=None, shap_mode='kernel', use_shap=True):\n",
    "        device = Xb.device\n",
    "        Users = Xb[:, 0].long()\n",
    "        Items = Xb[:, 1].long()\n",
    "        SideFeat_info = Xb[:, 2:]\n",
    "\n",
    "        User_EmbV = self.Emb_Dropout(self.Emb_User(Users))\n",
    "        Item_EmbV = self.Emb_Dropout(self.Emb_Item(Items))\n",
    "        EmbV_Concat = torch.cat([User_EmbV, Item_EmbV], dim=1)\n",
    "        EmbV_Proj = self.Emb_Projection(EmbV_Concat)\n",
    "\n",
    "        Out1 = self.LinC1(SideFeat_info)\n",
    "        Cat_Out = torch.cat([EmbV_Proj, Out1], dim=1)\n",
    "        Gamma1 = self.Film_Gamma1(Cat_Out)\n",
    "        Gamma2 = self.Film_Gamma2(Cat_Out)\n",
    "        Beta = self.Film_Beta(Cat_Out)\n",
    "\n",
    "        Aggregated_Film = Gamma1 * EmbV_Proj + Gamma2 * Out1 + Beta\n",
    "        Out2 = self.LinC2(Aggregated_Film)\n",
    "        Out_Rat = self.LinC3(Out2)\n",
    "        Out_Rat = torch.clamp(Out_Rat, 1.0, 5.0)\n",
    "\n",
    "        if Yb is not None:\n",
    "            Mse_Loss = self.criterion(Out_Rat, Yb)\n",
    "            Mae_Loss = nn.L1Loss()(Out_Rat, Yb)\n",
    "            Rmse_Loss = torch.sqrt(Mse_Loss)\n",
    "\n",
    "            Shap_Values = torch.zeros_like(SideFeat_info)\n",
    "            Penalty = torch.tensor(0.0, device = Out_Rat.device)\n",
    "\n",
    "            if use_shap and shap_mode == 'kernel':\n",
    "                try:\n",
    "                    self.eval()\n",
    "                    SideFeat_info_np = SideFeat_info.detach().cpu().numpy()\n",
    "                    Background = SideFeat_info_np[np.random.choice(len(SideFeat_info_np), min(50, len(SideFeat_info_np)), replace=False)]\n",
    "\n",
    "                    explainer = shap.KernelExplainer(lambda x: self.predict_fn(x, device), Background)\n",
    "                    Shap_Values_np = explainer.shap_values(SideFeat_info_np, nsamples=100)\n",
    "                    Shap_Values = torch.tensor(Shap_Values_np, dtype=torch.float32, device=Out_Rat.device)\n",
    "\n",
    "                    Negative_Shap = torch.clamp(Shap_Values, max=0.0)\n",
    "                    Negative_ShapA = Negative_Shap.abs()\n",
    "                    Num_Negative_Shap = (Negative_Shap < 0).sum(dim=1).float()\n",
    "                    Per_Sample_Penalty = Negative_ShapA.sum(dim=1) / (Num_Negative_Shap + 1e-6) / Shap_Values.shape[1]\n",
    "                    Penalty = Per_Sample_Penalty.mean()\n",
    "                    #print(\"Kernel SHAP Penalty:\", Penalty.item())\n",
    "                except Exception as e:\n",
    "                    print(\"Kernel SHAP failed:\", e)\n",
    "\n",
    "            return Out_Rat, Rmse_Loss, Mae_Loss, Shap_Values, Penalty\n",
    "\n",
    "        return Out_Rat\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afd65a-11dd-4e06-a55a-dff440aa2fc2",
   "metadata": {},
   "source": [
    "## For Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c373ff7-863a-44c2-ad8e-0722c2da3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_cfdata(Dataset):\n",
    "    def __init__(self, D_X, D_Y, DenseCols):\n",
    "        self.D_X = D_X\n",
    "        self.D_Y = D_Y\n",
    "        self.DenseCols = DenseCols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.D_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.D_X.iloc[idx].values.astype(np.float32)\n",
    "        y = np.array([self.D_Y.iloc[idx]], dtype=np.float32)\n",
    "        return torch.FloatTensor(x), torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a14f25-3593-4120-874f-1aebb381c9b1",
   "metadata": {},
   "source": [
    "## Mapping UserID and ItemID indices for training and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d605bf-e3cf-4f78-b467-55fefba2dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "User2idx = {uid: idx for idx, uid in enumerate(D_train['userId'].unique())}\n",
    "Item2idx = {iid: idx for idx, iid in enumerate(D_train['itemId'].unique())}\n",
    "\n",
    "D_train['user_idx'] = D_train['userId'].map(User2idx)\n",
    "D_train['item_idx'] = D_train['itemId'].map(Item2idx)\n",
    "\n",
    "D_valid = D_valid[D_valid['userId'].isin(User2idx) & D_valid['itemId'].isin(Item2idx)].copy()\n",
    "D_valid['user_idx'] = D_valid['userId'].map(User2idx)\n",
    "D_valid['item_idx'] = D_valid['itemId'].map(Item2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e91e0-a6c6-4949-8027-5347cd47594e",
   "metadata": {},
   "source": [
    "## Defining Relevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b48764-e2d2-4bd2-825b-119319c56b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseCols = D_train.columns[D_train.columns.str.startswith('A_')].tolist()\n",
    "InputCols = ['user_idx', 'item_idx'] + DenseCols\n",
    "\n",
    "DenseCols = D_train.columns[D_train.columns.str.startswith('A_')].tolist()\n",
    "InputCols = ['user_idx', 'item_idx'] + DenseCols\n",
    "\n",
    "scaler = StandardScaler()\n",
    "D_train[DenseCols] = scaler.fit_transform(D_train[DenseCols])\n",
    "D_valid[DenseCols] = scaler.transform(D_valid[DenseCols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccf249-cff5-4c76-854d-9434ca689f02",
   "metadata": {},
   "source": [
    "## Converting training data into float32 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f533c61-c8e3-4886-a338-658589be255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = D_train[InputCols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32).values\n",
    "y_train_np = D_train['rating'].astype(np.float32).values\n",
    "\n",
    "X_val_np = D_valid[InputCols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32).values\n",
    "y_val_np = D_valid['rating'].astype(np.float32).values\n",
    "\n",
    "X_train_Tens = torch.tensor(X_train_np)\n",
    "y_train_Tens = torch.tensor(y_train_np)\n",
    "\n",
    "X_val_Tens = torch.tensor(X_val_np)\n",
    "y_val_Tens = torch.tensor(y_val_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1effb9-c4fb-4d7f-9de4-5a6b1e7f35fc",
   "metadata": {},
   "source": [
    "## Defining Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f3ea7-3e89-4b2f-ac4b-4e0785f67fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = EmbFilm(N_Users=D_train['user_idx'].nunique(), N_Items=D_train['item_idx'].nunique(),\n",
    "           Dense_Size=len(DenseCols), Hidden=[20, 25, 10],\n",
    "           Dropout_emb=0.05, Dropouts=[0.2, 0.3, 0.2])\n",
    "\n",
    "num_weights = sum(p.numel() for p in Model.parameters())\n",
    "weight_shape = (num_weights,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b477f-4d04-4015-9e74-1f5ce86927fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = D_train[['user_idx', 'item_idx'] + DenseCols]\n",
    "X_valid = D_valid[['user_idx', 'item_idx'] + DenseCols]\n",
    "y_train = D_train['rating']\n",
    "y_valid = D_valid['rating']\n",
    "DL_train = DataLoader(Model_cfdata(X_train, y_train, DenseCols), batch_size=128, shuffle=True)\n",
    "DL_valid = DataLoader(Model_cfdata(X_valid, y_valid, DenseCols), batch_size=128)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162796c-3de9-4bbd-b181-d4878713736c",
   "metadata": {},
   "source": [
    "## Function for training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4dbc0-2034-4889-8c02-f7424ed50f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, Model, optimizer, device):\n",
    "        self.Model = Model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.Best_Rmse = float('inf')\n",
    "        self.Best_Mae = float('inf')\n",
    "\n",
    "        self.Loss_log = []\n",
    "        self.Rmse_log = []\n",
    "        self.Mae_log = []\n",
    "        self.Penalty_log = []\n",
    "\n",
    "    def compute_shap_penalty(self, Penalty, Lambda_Shap=1.0):\n",
    "        if isinstance(Penalty, torch.Tensor):\n",
    "            Mean_Val = Penalty.item()\n",
    "        else:\n",
    "            Mean_Val = float(Penalty)\n",
    "        return Lambda_Shap * Penalty\n",
    "\n",
    "    def fit(self, DL_train, DL_valid, n_epochs=5, Lambda_Shap=1.0):\n",
    "        for epoch in range(n_epochs):\n",
    "            self.Model.train()\n",
    "            Total_Loss = 0\n",
    "            preds_all, actual_all = [], []\n",
    "            epoch_penalty = 0.0\n",
    "            for Xb, Yb in DL_train:\n",
    "                Xb, Yb = Xb.to(self.device), Yb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                    \n",
    "                preds, Rmse_Loss, Mae_Loss, Shap_Values, Penalty = self.Model(Xb, Yb)  \n",
    "                shap_penalty = self.compute_shap_penalty(Penalty, Lambda_Shap=Lambda_Shap)\n",
    "                epoch_penalty += shap_penalty.item()\n",
    "\n",
    "                Loss = Rmse_Loss + Mae_Loss + shap_penalty\n",
    "                Loss = Loss/3.0\n",
    "                Loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                Total_Loss += Loss.item()\n",
    "                preds_all.extend(preds.detach().cpu().numpy())\n",
    "                actual_all.extend(Yb.cpu().numpy())\n",
    "\n",
    "            Rmse = np.sqrt(metrics.mean_squared_error(actual_all, preds_all))\n",
    "            Mae = metrics.mean_absolute_error(actual_all, preds_all)\n",
    "            self.Rmse_log.append(Rmse)\n",
    "            self.Mae_log.append(Mae)\n",
    "            self.Penalty_log.append(epoch_penalty)\n",
    "            self.Loss_log.append(Total_Loss/len(DL_train))\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Train Loss: {Total_Loss/len(DL_train):.4f}, RMSE: {Rmse:.4f}, MAE: {Mae:.4f}, Penalty: {self.Penalty_log[-1]:.4f}\")\n",
    "            self.evaluate(DL_valid)\n",
    "\n",
    "    def evaluate(self, DL_valid):\n",
    "            self.Model.eval()\n",
    "            preds_all, actual_all = [], []\n",
    "            with torch.no_grad():\n",
    "                for Xb, Yb in DL_valid:\n",
    "                    Xb, Yb = Xb.to(self.device), Yb.to(self.device)\n",
    "                    preds, _, _, _, _ = self.Model(Xb, Yb)  # Ignore SHAP during eval\n",
    "                    preds_all.extend(preds.cpu().numpy())\n",
    "                    actual_all.extend(Yb.cpu().numpy())\n",
    "        \n",
    "            D_valid['pred_rat'] = np.array(preds_all).flatten()\n",
    "            Rmse = np.sqrt(metrics.mean_squared_error(D_valid['rating'], D_valid['pred_rat']))\n",
    "            Mae = metrics.mean_absolute_error(D_valid['rating'], D_valid['pred_rat'])\n",
    "            R2 = metrics.r2_score(D_valid['rating'], D_valid['pred_rat'])\n",
    "            print(f\"Validation RMSE: {Rmse:.4f}, MAE: {Mae:.4f}, RÂ²: {R2:.4f}\")\n",
    "\n",
    "\n",
    "    def plot_logs(self):         \n",
    "            min_len = min(len(self.Rmse_log), len(self.Mae_log), len(self.Penalty_log), len(self.Loss_log))\n",
    "            epochs = list(range(1, min_len + 1))\n",
    "            rmse_log = self.Rmse_log[:min_len]\n",
    "            mae_log = self.Mae_log[:min_len]\n",
    "            penalty_log = self.Penalty_log[:min_len]\n",
    "            loss_log = self.Loss_log[:min_len]\n",
    "        \n",
    "            plt.figure(figsize=(15, 5))\n",
    "        \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(epochs, loss_log, label=\"Total Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Error\")\n",
    "            plt.title(\"Total Loss over Epochs\")\n",
    "            plt.legend()\n",
    "         \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(epochs, rmse_log, label=\"RMSE\")\n",
    "            plt.plot(epochs, mae_log, label=\"MAE\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Error\")\n",
    "            plt.title(\"RMSE and MAE over Epochs\")\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(epochs, penalty_log, color='purple')\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"SHAP Penalty\")\n",
    "            plt.title(\"Penalty from SHAP Constraint\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7053955-426a-4e69-8e4b-581677f92bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Learner_Model = Learner(Model, torch.optim.Adam(Model.parameters(), lr=5e-3, weight_decay=5e-4), device='cuda:0')\n",
    "Learner_Model.fit(DL_train, DL_valid, n_epochs=1, Lambda_Shap=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8d4a0-3c65-4683-b895-6bee05eae027",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner_Model.plot_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db973b19-df8c-4baa-9c87-d41dc1d6cffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
